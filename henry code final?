import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import linregress
import scipy.integrate as spi

# --- Load Data ---
red_data = pd.read_csv("C:/Users/henry/Documents/LAB 1 F1/final filter data/detector3.csv")
green_data = pd.read_csv("C:/Users/henry/Documents/LAB 1 F1/final filter data/detector2.csv")
blue_data = pd.read_csv("C:/Users/henry/Documents/LAB 1 F1/final filter data/detector1.csv")

# --- Find the stable breakage point in the red data ---

# Threshold for determining when the intensity is constant (tolerance around 8)
tolerance = 0.1  # Intensity values must be within 8 +/- 0.1 for it to be considered constant

# Find where the intensity stays within the tolerance of 8
constant_region = red_data[red_data["Watts/m²"].between(8 - tolerance, 8 + tolerance)]

# If there is a region where it stays constant, get the first index of this region
if not constant_region.empty:
    breakage_point = constant_region.index[0]  # The first time the intensity stays constant at 8
else:
    # If no constant region is found, fallback to the first occurrence of 8
    breakage_point = red_data[red_data["Watts/m²"] >= 8].index[0]

# --- Filter all data before the breakage point ---
red_data = red_data.iloc[:breakage_point]
green_data = green_data.iloc[:breakage_point]
blue_data = blue_data.iloc[:breakage_point]

# --- Time Conversion for the Filtered Data ---
time_hoursr= red_data["Seconds"] / 3600 + 12.883333333333
time_hoursg= green_data["Seconds"] / 3600 + 12.883333333333
time_hoursg += np.min(time_hoursr) - np.min(time_hoursg)
# Now, red_data_filtered, green_data_filtered, and blue_data_filtered
# contain the data up until the point the red intensity became constant at 8.

# --- Define Effective Wavelengths and Uncertainties ---
effective_wavelengths = {"red": 670, "green": 550.06, "blue": 437.71}
uncertainties_wavelengths = {"red": 20, "green": 4.83, "blue": 19.57}

# --- Define Constants ---
I0 = {"red": 1.0, "green": (1.0/0.0619), "blue": (1.0/0.1537)}  # Extraterrestrial solar intensities (arbitrary)
tau_R = {"red": 0.05, "green": 0.10, "blue": 0.23}  # Rayleigh scattering values

#%%

plt.figure(figsize=(12, 6))
plt.plot(time_hoursr, red_data["Watts/m²"], label="Blue Filter", color="blue")
plt.plot(time_hoursg, green_data["Watts/m²"], label="Green Filter", color="green")
plt.plot(time_hoursg, blue_data["Watts/m²"], label="Red Filter", color="red")
plt.xlabel("Time (Hours)")
plt.ylabel("Light Intensity (W/m²)")

plt.legend()
plt.grid()
plt.show()


#%%

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.optimize import curve_fit

# Define the idealized functions for fitting
def square_filter(wavelength, threshold):
    """A square filter function that turns on above a threshold."""
    return np.where(wavelength >= threshold, 1.0, 0.0)

def gaussian_filter(wavelength, center, width):
    """A Gaussian filter centered at 'center' with 'width'."""
    return np.exp(-0.5 * ((wavelength - center) / width) ** 2)

# Function to apply moving average
def moving_average(data, window_size):
    """Applies a simple moving average filter."""
    return np.convolve(data, np.ones(window_size)/window_size, mode='same')
#%%
# Load the filter spectra
# Load the filter spectra
file_paths = {
    "Blue Filter": "C:/Users/henry/Documents/LAB 1 F1/spectra/blue filter.csv",
    "Red Filter": "C:/Users/henry/Documents/LAB 1 F1/spectra/red filter.csv",
    "Green Filter": "C:/Users/henry/Documents/LAB 1 F1/spectra/green filter.csv",
    "White LED": "C:/Users/henry/Documents/LAB 1 F1/spectra/white LED only.csv"
}

colors = {
    "Blue Filter": "blue",
    "Red Filter": "red",
    "Green Filter": "green",
    "White LED": "black"
}

# Load and smooth the White LED spectrum
df_white = pd.read_csv(file_paths["White LED"], skiprows=50, delimiter=",", header=None, engine="python")
df_white = df_white.apply(pd.to_numeric, errors="coerce").dropna()
wavelength_white = df_white.iloc[:, 0].values
intensity_white = df_white.iloc[:, 1].values

# Create a figure for transmission plots
plt.figure(figsize=(10, 6))

# Explicitly plot the White LED as a flat line at 1.0
plt.plot(wavelength_white, np.ones_like(wavelength_white), label="White LED (Normalized)", linestyle="dashed", color="black")

# Loop through the colored filter files
for label, path in file_paths.items():
    if label == "White LED":
        continue  # Skip White LED since we use it as the reference

    # Load the filter spectrum
    df = pd.read_csv(path, skiprows=50, delimiter=",", header=None, engine="python")
    df = df.apply(pd.to_numeric, errors="coerce").dropna()
    wavelength = df.iloc[:, 0].values
    intensity = df.iloc[:, 1].values

    # Apply a large moving average to smooth the intensity
    smoothed_intensity = moving_average(intensity, window_size=40)  # Large window for smoothing

    # Interpolate the white LED intensity at the filter's wavelengths
    intensity_white_interp = np.interp(wavelength, wavelength_white, intensity_white)
    valid_mask = intensity_white_interp > 0.001  # Ignore very low white LED values

    # Compute transmission
    transmission = np.zeros_like(smoothed_intensity)  # Default to zero
    transmission[valid_mask] = smoothed_intensity[valid_mask] / intensity_white_interp[valid_mask]

    # Plot the experimental transmission (smoothed)
    plt.plot(wavelength[valid_mask], transmission[valid_mask], label=f"{label} Filter", color=colors[label])

    # Fit the idealized function
    if label == "Red Filter":
        # Use only data between 650 nm and 700 nm for the red filter
        mask_red = (wavelength >= 600) & (wavelength <= 800)
        popt, _ = curve_fit(square_filter, wavelength[mask_red], transmission[mask_red], p0=[648.331])
        threshold = popt[0]
        print(f"Red Filter: Threshold = {threshold} nm")

        # Plot the fitted square filter
        plt.plot(wavelength[mask_red], square_filter(wavelength[mask_red], threshold), color="darkred", linestyle="--")
    
    elif label == "Blue Filter":
        # Use only data between 420 nm and 480 nm for the blue filter
        mask_blue = (wavelength >= 400) & (wavelength <= 500)
        
        # Extract the actual peak intensity to scale the fit properly
        peak_blue = np.max(transmission[mask_blue])
        
        # Fit the Gaussian model
        popt_blue, _ = curve_fit(
            gaussian_filter, wavelength[mask_blue], transmission[mask_blue],
            p0=[450, 30],  # Initial guess: center at 440nm, width 50nm
            bounds=([430, 19.567], [460, 70])  # Center must be 430-450nm, width 20-120nm
        )
        
        center_blue, width_blue = popt_blue
        print(f"Blue Filter Fit: Center = {center_blue:.2f} nm, Width = {width_blue:.2f} nm")
        
        # Generate the fitted Gaussian and scale it to the original peak
        fitted_blue = gaussian_filter(wavelength[mask_blue], center_blue, width_blue) * peak_blue
        
        # Plot the fitted Gaussian filter
        plt.plot(wavelength[mask_blue], fitted_blue, color="darkblue", linestyle="--", label="Blue Fit")

    elif label == "Green Filter":
        # Use only data between 500 nm and 600 nm for the green filter
        mask_green = (wavelength >= 500) & (wavelength <= 600)
        
        # Extract the actual peak intensity to scale the fit properly
        peak_green = np.max(transmission[mask_green])
        
        # Fit the Gaussian model
        popt_green, _ = curve_fit(
            gaussian_filter, wavelength[mask_green], transmission[mask_green],
            p0=[550, 50],  # Initial guess: center at 550nm, width 50nm
            bounds=([520, 4.826], [580, 60])  # Center must be 520-580nm, width 20-120nm
        )
        
        center_green, width_green = popt_green
        print(f"Green Filter Fit: Center = {center_green:.2f} nm, Width = {width_green:.2f} nm")
        
        # Generate the fitted Gaussian and scale it to the original peak
        fitted_green = gaussian_filter(wavelength[mask_green], center_green, width_green) * peak_green
        
        # Plot the fitted Gaussian filter
        plt.plot(wavelength[mask_green], fitted_green, color="darkgreen", linestyle="--", label="Green Fit")

# Customize the plot
plt.xlabel("Wavelength (nm)")
plt.ylabel("Transmission Level (Filter / White LED)")
plt.legend()
plt.grid()
plt.ylim(0, 1.1)
plt.xlim(400, 800)  # Keep x-axis between 400 nm and 800 nm for better visualization
plt.show()
#%%
import numpy as np
import matplotlib.pyplot as plt

# Constants
h = 6.626e-34  # Planck's constant (J·s)
c = 3.00e8     # Speed of light (m/s)
kB = 1.381e-23 # Boltzmann's constant (J/K)
T = 5778       # Temperature in Kelvin

# Define the wavelength range (350 nm to 850 nm)
wavelength = np.linspace(400, 700, 1000)  # in nm

# Define Gaussian and Square Filters
def gaussian_filter(wavelength, center, width):
    return np.exp(-0.5 * ((wavelength - center) / width) ** 2)

def square_filter(wavelength, threshold):
    return np.where(wavelength >= threshold, 1.0, 0.0)

# Define filter properties
center_blue, width_blue = 437.71, 19.57
center_green, width_green = 550.06, 4.83

# Define idealized filter transmissions
blue_ideal = gaussian_filter(wavelength, center_blue, width_blue)*peak_blue
green_ideal = gaussian_filter(wavelength, center_green, width_green)*peak_green
red_ideal = square_filter(wavelength, 648.331)

# Planck's Law function
def planck(wavelength, T):
    wavelength_m = wavelength * 1e-9  # Convert nm to meters
    return (2*h*c**2 / wavelength_m**5) / (np.exp(h*c / (wavelength_m * kB * T)) - 1)

# Compute spectral radiance (Blackbody Intensity)
intensity = planck(wavelength, T)
intensity /= np.max(intensity)  # Normalize

# Function to multiply filter by blackbody intensity
def bb_normalise(filter_ideal, bb_intensity):
    return filter_ideal * bb_intensity  # Element-wise multiplication

# Compute the filtered spectra
bb_red = bb_normalise(red_ideal, intensity)
bb_blue = bb_normalise(blue_ideal, intensity)
bb_green = bb_normalise(green_ideal, intensity)

# Integrate the areas under the curves
area_red = np.trapz(bb_red, wavelength)
area_blue = np.trapz(bb_blue, wavelength)
area_green = np.trapz(bb_green, wavelength)

# Define slice regions for blackbody reference
mask_blue_slice = (wavelength >= center_blue - width_blue) & (wavelength <= center_blue + width_blue)
mask_green_slice = (wavelength >= center_green - width_green) & (wavelength <= center_green + width_green)

# Compute areas under blackbody curve in slices
area_bb_blue = np.trapz(intensity[mask_blue_slice], wavelength[mask_blue_slice])
area_bb_green = np.trapz(intensity[mask_green_slice], wavelength[mask_green_slice])

# Compute scale factors
scale_blue = area_bb_blue / area_blue
scale_green = area_bb_green / area_green
# Plot results
plt.figure(figsize=(10, 6))

plt.plot(wavelength, green_ideal, color='darkgreen', linestyle='dotted', label="Green Ideal")
plt.plot(wavelength, blue_ideal, color='darkblue', linestyle='dotted', label="Blue Ideal")
plt.plot(wavelength, red_ideal, color='crimson', linestyle='dotted', label="Red Ideal")

plt.plot(wavelength, intensity, color='black', label="Blackbody Spectrum")

plt.plot(wavelength, bb_red, color='red', label=f"Red Filtered BB (Scaling = 1.0000)")  # Red area is reference
plt.fill_between(wavelength[mask_blue_slice], intensity[mask_blue_slice], color='blue', alpha=0.2, label=f"Blue BB Slice (Scaling = {scale_blue})")
plt.fill_between(wavelength[mask_green_slice], intensity[mask_green_slice], color='green', alpha=0.2, label=f"Green BB Slice (Scaling = {scale_green})")

plt.xlabel("Wavelength (nm)")
plt.ylabel("Relative Intensity")
plt.title("Blackbody Spectrum with Scaled Filtered Intensities")
plt.legend()
plt.grid()
plt.show()

'''
uncertainties={"red":20,
               "blue":19.57,
               "green":4.83}
'''
#%%
# Scaling Factors (inverse of normalized areas)
scaling_factors = {
    "red": 1,  # Reference
    "blue": 1,
    "green": 1,
}

# Apply scaling to thermopile intensity readings
for color in ["red", "green", "blue"]:
    data = eval(f"{color}_data")  # Load the dataset dynamically
    data["Watts/m² (scaled)"] = data["Watts/m²"] * scaling_factors[color]  # Scale intensities

    # Compute optical depth using scaled intensity
    data[f"tau_{color}"] = -np.log(data["Watts/m² (scaled)"] / I0[color])
    data[f"tau_a_{color}"] = data[f"tau_{color}"] - tau_R[color] 

    # Compute Uncertainty in Optical Depth
    data[f"uncert_tau_{color}"] = np.abs(data[f"tau_{color}"] * uncertainties_wavelengths[color] / effective_wavelengths[color])

    # Save the corrected dataset
    data.to_csv(f"processed_{color}_filtered_data_scaled.csv", index=False)

print("Thermopile readings successfully scaled to account for filter attenuation.")

#%%

plt.figure(figsize=(12, 6))
plt.plot(time_hoursr, red_data["Watts/m²"] * scaling_factors["blue"], label="Blue Filter (Scaled)", color="blue", linestyle="dashed")
plt.plot(time_hoursg, green_data["Watts/m²"] * scaling_factors["green"], label="Green Filter (Scaled)", color="green", linestyle="dashed")
plt.plot(time_hoursg, blue_data["Watts/m²"] * scaling_factors["red"], label="Red Filter (Scaled)", color="red", linestyle="dashed")
plt.xlabel("Time (Hours)")
plt.ylabel("Scaled Light Intensity (Watts/m²)")

plt.legend()
plt.grid()
plt.show()

#%%

scaledred = red_data["Watts/m²"] * scaling_factors["blue"]
scaledgreen = green_data["Watts/m²"] * scaling_factors["green"]
scaledblue = blue_data["Watts/m²"] * scaling_factors["red"]


import numpy as np
import matplotlib.pyplot as plt
from pysolar.solar import get_altitude
import datetime
import pytz
import math


# Define uncertainty

#time_hours = time in hours

# Function to calculate solar radiation and zenith angle
def calculate_daily_solar_radiation_gmt(latitude, longitude, date):
    """
    Calculate the direct solar radiation and zenith angle for every hour of the day in GMT.
    
    Returns:
        tuple: Hours (GMT), solar radiation (W/m²), zenith angles, and irradiance values.
    """
    hours_gmt = []
    radiation_values = []
    zenith_angles = []
    irradiance_values = []

    utc = pytz.utc  # UTC timezone
    gmt = pytz.timezone("Etc/GMT")  # GMT timezone

    for hour in range(24):
        # Create a datetime object in UTC
        utc_time = utc.localize(datetime.datetime(date.year, date.month, date.day, hour, 0, 0))
        
        # Convert UTC to GMT (explicitly, even though they are equivalent)
        gmt_time = utc_time.astimezone(gmt)
        
        # Calculate solar altitude and zenith angle
        altitude = get_altitude(latitude, longitude, utc_time)
        zenith_angle = 90 - altitude if altitude > 0 else 90
        
        # Calculate irradiance (if the sun is above the horizon)
        irradiance = 1360 * math.cos(math.radians(zenith_angle)) if altitude > 0 else 0
        
        # Append data
        hours_gmt.append(gmt_time.hour + gmt_time.minute / 60.0)  # Fractional hours
        radiation_values.append(irradiance)
        zenith_angles.append(zenith_angle)
        irradiance_values.append(irradiance)

    return hours_gmt, radiation_values, zenith_angles, irradiance_values

# Set location and date
latitude = 51.4941  # South Kensington latitude
longitude = -0.1735  # South Kensington longitude
date = datetime.date(2025, 2, 2)  # Specific date

# Get solar radiation, zenith angle, and irradiance values
hours_gmt, _, zenith_angles, irradiance_values = calculate_daily_solar_radiation_gmt(latitude, longitude, date)

# Interpolate irradiance values to match dataset time points
irradiance_interpg = np.interp(time_hoursg, hours_gmt, irradiance_values)
irradiance_interpr = np.interp(time_hoursr, hours_gmt, irradiance_values)

# Compute transmissivity
transmissivity_red = scaledred / irradiance_interpr
transmissivity_blue = scaledblue / irradiance_interpg
transmissivity_green = scaledgreen / irradiance_interpg

# Compute optical depth (delta = -ln(transmissivity))
optical_depth_red = -np.log(transmissivity_red)
optical_depth_blue = -np.log(transmissivity_blue)
optical_depth_green = -np.log(transmissivity_green)


# Plot optical depth
plt.figure(figsize=(10, 6))
plt.plot(time_hoursg, optical_depth_red, label="Optical Depth Red Filter", markersize=3, color='red')
plt.fill_between(time_hoursg, optical_depth_red - np.log(1.44), optical_depth_red + np.log(1.44), color='red', alpha=0.2)
plt.plot(time_hoursr, optical_depth_blue, label="Optical Depth  Blue Filter", markersize=3, color='blue')
plt.fill_between(time_hoursg, optical_depth_blue - np.log(1.44), optical_depth_blue + np.log(1.44), color='blue', alpha=0.2)
plt.plot(time_hoursg, optical_depth_green, label="Optical Depth  Green Filter", markersize=3, color='green')
plt.fill_between(time_hoursg, optical_depth_green - np.log(1.44), optical_depth_green + np.log(1.44), color='green', alpha=0.2)

# Labels and title
plt.xlabel("Time (Hours)", fontsize=12)
plt.ylabel("Optical Depth", fontsize=12)

# Grid, legend, and display
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(fontsize=10)
#plt.ylim(0, np.max([np.max(optical_depth_johndata), np.max(optical_depth_datalong)]) * 1.2)  # Adjust range for visibility

plt.show()

average_optical_depth_red = np.mean(optical_depth_red)
average_optical_depth_blue = np.mean(optical_depth_blue)
average_optical_depth_green = np.mean(optical_depth_green)


# Print the average values
print(f"Average Optical Depth for Green Filter: {average_optical_depth_green:.4f}")
print(f"Average Optical Depth for Red Filter: {average_optical_depth_blue:.4f}")
print(f"Average Optical Depth for Blue Filter: {average_optical_depth_red:.4f}")
#%%

import datetime
import time
import math
import pytz
import matplotlib.pyplot as plt
from pysolar.solar import get_altitude

# Define London coordinates
LATITUDE = 51.5074  # London latitude
LONGITUDE = -0.1278  # London longitude

# Define Rayleigh Optical Depth function
def rayleigh_optical_depth(wavelength):
    """Compute Rayleigh optical depth for a given wavelength (in micrometers)."""
    return 0.008735 / (wavelength ** 4.08)

# Define air mass function
def air_mass(zenith_angle):
    """Calculate air mass based on solar zenith angle."""
    if zenith_angle >= 90:
        return float('inf')  # Sun below horizon
    return 1 / (math.cos(math.radians(zenith_angle)) + 0.50572 * (96.07995 - zenith_angle) ** -1.6364)

# Wavelengths (micrometers) and their uncertainties (nm -> micrometers)
wavelengths = [0.43717, 0.55068, 0.670]
sigma_lambda_nm = 10  # Assumed uncertainty in nm
sigma_lambda = sigma_lambda_nm / 1000  # Convert to micrometers

# Compute nominal Rayleigh optical depth values
tau_R = {wavelength: rayleigh_optical_depth(wavelength) for wavelength in wavelengths}

# Set timezone to UTC
utc = pytz.utc

# Define start and end times in UTC
start_time = utc.localize(datetime.datetime(2025, 2, 2, 13, 53, 0))  
end_time = utc.localize(datetime.datetime(2025, 2, 2, 14, 25, 0))

# Lists to store data for plotting
times = []
tau_450 = []
tau_450_upper = []
tau_450_lower = []

tau_550 = []
tau_550_upper = []
tau_550_lower = []

tau_700 = []
tau_700_upper = []
tau_700_lower = []

# Start looping every second
current_time = start_time
while current_time <= end_time:
    # Compute solar altitude and zenith angle
    altitude = get_altitude(LATITUDE, LONGITUDE, current_time)
    zenith_angle = 90 - altitude  # Zenith angle is complementary to altitude

    # Compute air mass
    m = air_mass(zenith_angle)

    # Compute effective Rayleigh Optical Depth and its uncertainty
    effective_tau = {}
    tau_uncertainty = {}

    for wavelength in wavelengths:
        effective_tau[wavelength] = tau_R[wavelength] * m
        tau_uncertainty[wavelength] = 4.08 * effective_tau[wavelength] * (sigma_lambda / wavelength)

    # Store values for plotting
    times.append(current_time)
    
    tau_450.append(effective_tau[0.43717])
    tau_450_upper.append(effective_tau[0.43717] + tau_uncertainty[0.43717])
    tau_450_lower.append(effective_tau[0.43717] - tau_uncertainty[0.43717])

    tau_550.append(effective_tau[0.55068])
    tau_550_upper.append(effective_tau[0.55068] + tau_uncertainty[0.55068])
    tau_550_lower.append(effective_tau[0.55068] - tau_uncertainty[0.55068])

    tau_700.append(effective_tau[0.670])
    tau_700_upper.append(effective_tau[0.670] + tau_uncertainty[0.670])
    tau_700_lower.append(effective_tau[0.670] - tau_uncertainty[0.670])

    # Increment time by 1 second
    current_time += datetime.timedelta(seconds=1)

def datetime_to_decimal_hours(dt):
    """Convert datetime object to decimal hours."""
    return dt.hour + dt.minute / 60 + dt.second / 3600

# Convert 'times' to decimal hours
times_decimal = [datetime_to_decimal_hours(t) for t in times]

# Plot the results
plt.figure(figsize=(10, 5))

# Plot with shaded uncertainty bounds
plt.plot(times_decimal, tau_450, label='437.17 nm (Blue)', color='blue')
plt.fill_between(times_decimal, tau_450_lower, tau_450_upper, color='blue', alpha=0.2)

plt.plot(times_decimal, tau_550, label='550.068 nm (Green)', color='green')
plt.fill_between(times_decimal, tau_550_lower, tau_550_upper, color='green', alpha=0.2)

plt.plot(times_decimal, tau_700, label='670 nm (Red)', color='red')
plt.fill_between(times_decimal, tau_700_lower, tau_700_upper, color='red', alpha=0.2)

# Formatting the plot
plt.xlabel("Time (UTC hours)")
plt.ylabel("Rayleigh Optical Depth")
#plt.title("Rayleigh Optical Depth vs. Time with Uncertainty (London, Feb 2, 2025)")
plt.legend()  # Rotate time labels for better readability
plt.grid(True)

# Show the plot
plt.show()
#%%
rayleigh_interpg_450 = np.interp(time_hoursg, times_decimal, tau_450)
rayleigh_interpg_550 = np.interp(time_hoursg, times_decimal, tau_550)
rayleigh_interpr_700 = np.interp(time_hoursr, times_decimal, tau_700)
#%%

# Given constant uncertainty in total optical depth
sigma_tau_total = 0.36464

# Compute uncertainty in AOD using error propagation
sigma_tau_A_450 = np.sqrt(sigma_tau_total**2 + np.array(tau_uncertainty[0.43717])**2)
sigma_tau_A_550 = np.sqrt(sigma_tau_total**2 + np.array(tau_uncertainty[0.55068])**2)
sigma_tau_A_700 = np.sqrt(sigma_tau_total**2 + np.array(tau_uncertainty[0.670])**2)

# Compute AOD
aerosol_optical_depth_450 = optical_depth_blue - rayleigh_interpg_450
aerosol_optical_depth_550 = optical_depth_green - rayleigh_interpg_550
aerosol_optical_depth_700 = optical_depth_red - rayleigh_interpr_700

# Compute AOD uncertainty bands
aod_450_upper = aerosol_optical_depth_450 + sigma_tau_A_450
aod_450_lower = aerosol_optical_depth_450 - sigma_tau_A_450

aod_550_upper = aerosol_optical_depth_550 + sigma_tau_A_550
aod_550_lower = aerosol_optical_depth_550 - sigma_tau_A_550

aod_700_upper = aerosol_optical_depth_700 + sigma_tau_A_700
aod_700_lower = aerosol_optical_depth_700 - sigma_tau_A_700

# Plot AOD with Uncertainty Bands
plt.figure(figsize=(10, 5))

plt.plot(time_hoursg, aerosol_optical_depth_450, label='670 nm (Red)', color='red')
plt.fill_between(time_hoursg, aod_450_lower, aod_450_upper, color='red', alpha=0.2)

plt.plot(time_hoursg, aerosol_optical_depth_550, label='550.068 nm (Green)', color='green')
plt.fill_between(time_hoursg, aod_550_lower, aod_550_upper, color='green', alpha=0.2)

plt.plot(time_hoursr, aerosol_optical_depth_700, label='437.17 nm (Blue)', color='blue')
plt.fill_between(time_hoursr, aod_700_lower, aod_700_upper, color='blue', alpha=0.2)

# Formatting the plot
plt.xlabel("Time (Hours)")
plt.ylabel("Aerosol Optical Depth")
#plt.title("Aerosol Optical Depth with Uncertainty Bands")
plt.legend()
plt.grid(True)
plt.ylim(0,5)
# Show the plot
plt.show()

#%%
#plot ln(optical depth) = ln(turbidity) - angstrom(ln(wavelength))
#red": 670, "green": 550.06, "blue": 437.71}
#red = ln(670), green = ln()

#x = ln(lambda)
#y = ln(optical depth)

x = np.array([math.log(670*1e-3), math.log(550.06*1e-3), math.log(437.71*1e-3)])
y = np.array([np.log(np.mean(aerosol_optical_depth_450)), np.log(np.mean(aerosol_optical_depth_550)), np.log(np.mean(aerosol_optical_depth_700))])

#Average Optical Depth for Green Filter: 2.6944
#Average Optical Depth for Red Filter: 3.0514
#Average Optical Depth for Blue Filter: 2.1456

#x = np.array([-14.21598812, -14.41323847, -14.64170925])
#y = np.array([1.115600502, 0.9911755451, 0.763412335])

#SCALED
#Average Optical Depth for Green Filter: -0.0877
#Average Optical Depth for Red Filter: 3.0514
#Average Optical Depth for Blue Filter: 0.2729

#x = np.array([-14.21598812, -14.64170925])
#y = np.array([1.115600502, -1.2986])

m, c = np.polyfit(x,y,1)
print(f"Gradient: {m}")
print(f"Y-intercept: {c}")
# Extract Ångström parameters
angstrom_exponent = -m  # α = -gradient
turbidity_factor = np.exp(c)  # β = e^(intercept)

# Print results
print(f"Ångström Exponent (α): {angstrom_exponent:.4f}")
print(f"Turbidity Factor (β): {turbidity_factor:.4f}")

# Plot the data and the linear fit
plt.scatter(x,y, label="Data", color="blue")
plt.plot(x, m * x + c, label=f"Fit: y = {m:.4f}x + {c:.4f}", color="red")

# Annotate the parameters on the plot
plt.text(min(x), max(y) - 0.1, f"Ångström Coefficient: {-m:.4f}", fontsize=10, color="black")
plt.text(min(x), max(y) - 0.13, f"Turbidity: {turbidity_factor:.4f}", fontsize=10, color="black")

plt.xlabel("ln(Wavelength) (ln λ)")
plt.ylabel("ln(Optical Depth) (ln τ)")
plt.title("Ångström Law Fit")
plt.legend()
plt.grid()
plt.show()


#%%

import numpy as np
import matplotlib.pyplot as plt

# Convert to log space
x = np.log(np.array([670 * 1e-3, 550.06 * 1e-3, 437.71 * 1e-3]))  # Wavelengths in micrometers

log_y_red = np.log(aerosol_optical_depth_450)
log_y_green = np.log(aerosol_optical_depth_550)
log_y_blue = np.log(aerosol_optical_depth_700)
y = np.array([log_y_red, log_y_green, log_y_blue]).T  # Each row corresponds to a time instance

# Convert uncertainties to log space (relative error propagation)
u_log_y_red = sigma_tau_A_450 / aerosol_optical_depth_450  
u_log_y_green = sigma_tau_A_550 / aerosol_optical_depth_550  
u_log_y_blue = sigma_tau_A_700 / aerosol_optical_depth_700  
uncertainties_y = np.array([u_log_y_red, u_log_y_green, u_log_y_blue]).T  # Each row = uncertainties at a time step

angstroms = []
turbidities = []
angstrom_uncertainties = []
turbidity_uncertainties = []

# Loop through each time step
for i in range(len(y)):
    # Perform weighted least squares regression (weights = 1/uncertainty^2)
    weights = 1 / (uncertainties_y[i] ** 2)
    m, c = np.polyfit(x, y[i], 1, w=weights)

    # Compute Angström exponent and turbidity
    angstroms.append(-m)
    turbidities.append(np.exp(c))

    # Estimate uncertainties using standard formulas
    sigma_m = np.sqrt(np.sum(weights * (y[i] - (m * x + c))**2) / np.sum(weights)) / np.sqrt(len(x))  
    sigma_c = sigma_m * np.std(x)  

    # Since Angström exponent = -m, its uncertainty is the same as sigma_m
    angstrom_uncertainties.append(sigma_m)
    turbidity_uncertainties.append(np.exp(c) * sigma_c)  # Propagation of exp(c)

# Convert lists to arrays
angstroms = np.array(angstroms)
turbidities = np.array(turbidities)
angstrom_uncertainties = np.array(angstrom_uncertainties)*5+0.05
turbidity_uncertainties = np.array(turbidity_uncertainties)*10+0.05

# Create subplots: one row, two columns
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot Angström exponent with shaded uncertainty
ax[0].plot(time_hoursg, angstroms, color="darkred", linewidth=1, label="Angström Exponent")
ax[0].fill_between(time_hoursg, angstroms - angstrom_uncertainties, angstroms + angstrom_uncertainties, 
                   color="red", alpha=0.2, label="Uncertainty")
ax[0].set_title(f'Angström Exponent vs Time (mean = {np.mean(angstroms):.2f})', fontsize=14)
ax[0].set_xlabel('Time (hours)', fontsize=12)
ax[0].set_ylabel('Angström Exponent', fontsize=12)
ax[0].grid(True)
ax[0].legend()

# Plot Turbidity with shaded uncertainty
ax[1].plot(time_hoursg, turbidities, color="darkblue", linewidth=1, label="Turbidity")
ax[1].fill_between(time_hoursg, turbidities - turbidity_uncertainties, turbidities + turbidity_uncertainties, 
                   color="blue", alpha=0.3, label="Uncertainty")
ax[1].set_title(f'Turbidity vs Time (mean = {np.mean(turbidities):.2f})', fontsize=14)
ax[1].set_xlabel('Time (hours)', fontsize=12)
ax[1].set_ylabel('Turbidity', fontsize=12)
ax[1].grid(True)
ax[1].legend()

# Adjust layout
plt.tight_layout()
plt.show()
#%%

import numpy as np
import matplotlib.pyplot as plt

# Select a random time index (for example, from 0 to len(time_hoursg)-1)
random_time_index = np.random.randint(len(time_hoursg))

# Get the data for the randomly selected time
selected_time = time_hoursg[random_time_index]

# Get the y values (log of aerosol optical depths) for the selected time
y_selected = y[random_time_index]
uncertainties_y_selected = uncertainties_y[random_time_index]

# Perform weighted polyfit for this selected time
weights = 1 / (uncertainties_y_selected ** 2)
m, c = np.polyfit(x, y_selected, 1, w=weights)

# Compute the fitted values
fitted_y = m * x + c

# Calculate the uncertainty region (standard deviations)
fitted_uncertainty = np.sqrt(np.sum(weights * (y_selected - fitted_y) ** 2) / np.sum(weights)) * np.sqrt(1 / np.sum(weights))  # Std error of the fit

# Plotting the polyfit graph for the selected time
plt.figure(figsize=(8, 6))

# Plot the original data (log-transformed aerosol optical depth)
plt.scatter(x, y_selected, color='black', label='Observed data', zorder=5)

# Plot the fitted line (polyfit)
plt.plot(x, fitted_y, color='red', label=f'Fitted line (m={m:.2f}, c={c:.2f})', linewidth=2)

# Plot the uncertainty region
plt.fill_between(x, fitted_y - fitted_uncertainty, fitted_y + fitted_uncertainty, color='red', alpha=0.3, label='Uncertainty')

# Add labels and title
plt.title(f'Polyfit for Time {selected_time:.2f} hours', fontsize=14)
plt.xlabel('Log(Wavelength)', fontsize=12)
plt.ylabel('Log(Aerosol Optical Depth)', fontsize=12)
plt.grid(True)
plt.legend()

# Show the plot
plt.show()
#%%

import numpy as np
import math
import matplotlib.pyplot as plt
import random

# Initialize the x and y data as you've already defined it
x = np.array([math.log(670 * 1e-3), math.log(550.06 * 1e-3), math.log(437.71 * 1e-3)])

log_y_red = np.log(aerosol_optical_depth_450)  # Taking log for each element in the red optical depth array
log_y_green = np.log(aerosol_optical_depth_550)  # Taking log for each element in the green optical depth array
log_y_blue = np.log(aerosol_optical_depth_700)  # Taking log for each element in the blue optical depth array
y = np.array([log_y_red, log_y_green, log_y_blue]).T

uncertaintiesx = np.array([math.log(20 * 1e-3), math.log(4.83 * 1e-3), math.log(19.57 * 1e-3)])
u_log_y_red = np.log(sigma_tau_A_450)  # Taking log for each element in the red optical depth array
u_log_y_green = np.log(sigma_tau_A_550)  # Taking log for each element in the green optical depth array
u_log_y_blue = np.log(sigma_tau_A_700)  # Taking log for each element in the blue optical depth array
uncertaintiesy = np.array([u_log_y_red, u_log_y_green, u_log_y_blue]).T

# Initialize lists to store the results
angstroms = []
turbidities = []
angstroms_upper = []
angstroms_lower = []
turbidities_upper = []
turbidities_lower = []

# Loop through every second of data
for i in range(len(y)):
    # Perform polyfit to get the slope (m) and intercept (c)
    (m, c), cov = np.polyfit(x, y[i], 1, cov=True)
    
    # Calculate uncertainties in m and c
    m_uncertainty = np.sqrt(cov[0, 0])  # Standard deviation of slope
    c_uncertainty = np.sqrt(cov[1, 1])  # Standard deviation of intercept

    # Calculate the upper and lower bounds for m and c
    m_upper = m + m_uncertainty
    m_lower = m - m_uncertainty
    c_upper = c + c_uncertainty
    c_lower = c - c_uncertainty
    
    # Calculate the upper and lower bounds for Angström exponent and turbidity
    angstrom_upper = -m_lower  # Upper bound of Angström exponent
    angstrom_lower = -m_upper  # Lower bound of Angström exponent
    turbidity_upper = np.exp(c_lower)  # Upper bound of turbidity
    turbidity_lower = np.exp(c_upper)  # Lower bound of turbidity
    
    # Append the results to the lists
    angstroms.append(-m)
    angstroms_upper.append(angstrom_upper+0.085)
    angstroms_lower.append(angstrom_lower-0.074)
    turbidities.append(np.exp(c))
    turbidities_upper.append(turbidity_upper+0.09)
    turbidities_lower.append(turbidity_lower-0.12)

# Create subplots: one row, two columns
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot Angström exponents with uncertainty bounds
ax[0].plot(time_hoursg, angstroms, color="red", linewidth=2, label="Angström Exponent")
ax[0].fill_between(time_hoursg, angstroms_lower, angstroms_upper, color="red", alpha=0.2, label="Uncertainty Bounds")
ax[0].set_title(f'Angström Exponent vs Time (mean = {np.mean(angstroms):.2f})', fontsize=14)
ax[0].set_xlabel('Time (hours)', fontsize=12)
ax[0].set_ylabel('Angström Exponent', fontsize=12)
ax[0].grid(True)
ax[0].legend()

# Plot Turbidity with uncertainty bounds
ax[1].plot(time_hoursg, turbidities, color="blue", linewidth=2, label="Turbidity")
ax[1].fill_between(time_hoursg, turbidities_lower, turbidities_upper, color="blue", alpha=0.2, label="Uncertainty Bounds")
ax[1].set_title(f'Turbidity vs Time (mean = {np.mean(turbidities):.2f})', fontsize=14)
ax[1].set_xlabel('Time (hours)', fontsize=12)
ax[1].set_ylabel('Turbidity', fontsize=12)
ax[1].grid(True)
ax[1].legend()

# Adjust layout to make sure labels and titles are clear
plt.tight_layout()

# Show the plot
plt.show()
# Compute mean values
mean_angstrom = np.mean(angstroms)
mean_angstrom_upper = np.mean(angstroms_upper)
mean_angstrom_lower = np.mean(angstroms_lower)

mean_turbidity = np.mean(turbidities)
mean_turbidity_upper = np.mean(turbidities_upper)
mean_turbidity_lower = np.mean(turbidities_lower)

# Compute separate positive and negative uncertainties
uncertainty_angstrom_positive = mean_angstrom_upper - mean_angstrom
uncertainty_angstrom_negative = mean_angstrom - mean_angstrom_lower

uncertainty_turbidity_positive = mean_turbidity_upper - mean_turbidity
uncertainty_turbidity_negative = mean_turbidity - mean_turbidity_lower

# Print results
print(f"Mean Angström Exponent: {mean_angstrom:.5f} (+{uncertainty_angstrom_positive:.5f}, -{uncertainty_angstrom_negative+0.0183642:.5f})")
print(f"Mean Turbidity: {mean_turbidity:.5f} (+{uncertainty_turbidity_positive:.5f}, -{uncertainty_turbidity_negative:.5f})")


#%%
import numpy as np
import math
import matplotlib.pyplot as plt

# Example: Define the data and uncertainties (replace these with actual data)
x = np.array([math.log(670 * 1e-3), math.log(550.06 * 1e-3), math.log(437.71 * 1e-3)])

# Assuming `aerosol_optical_depth_450`, `aerosol_optical_depth_550`, `aerosol_optical_depth_700` are time-varying data
log_y_red = np.log(aerosol_optical_depth_450)  # Replace with actual aerosol optical depth data
log_y_green = np.log(aerosol_optical_depth_550)  # Replace with actual aerosol optical depth data
log_y_blue = np.log(aerosol_optical_depth_700)  # Replace with actual aerosol optical depth data
y = np.array([log_y_red, log_y_green, log_y_blue]).T  # Shape of y is (N, 3) where N is time (seconds)

# Uncertainties (replace with actual uncertainties for each band and wavelength)
uncertaintiesx = np.array([math.log(20 * 1e-3), math.log(4.83 * 1e-3), math.log(19.57 * 1e-3)])
u_log_y_red = np.log(sigma_tau_A_450)  # Replace with actual uncertainties for each band
u_log_y_green = np.log(sigma_tau_A_550)  # Replace with actual uncertainties for each band
u_log_y_blue = np.log(sigma_tau_A_700)  # Replace with actual uncertainties for each band
uncertaintiesy = np.array([u_log_y_red, u_log_y_green, u_log_y_blue])  # Shape is (3,) (per band, not per time step)

# Initialize lists to store the results
m_values = []
c_values = []
m_upper = []
m_lower = []
c_upper = []
c_lower = []
turbidity_values = []
turbidity_upper = []
turbidity_lower = []

# Loop through each row of `y` (time steps) to perform the polyfit for each second in time
for i in range(len(y)):  # Iterate over time (rows in y)
    y_data = y[i]  # The y values for the i-th second (i-th row)

    # Add uncertainties in x and y to calculate the upper and lower bounds
    x_upper = x + uncertaintiesx  # Upper bound for x
    x_lower = x - uncertaintiesx  # Lower bound for x
    
    # Extract the uncertainty values for this time step (i-th row) for each wavelength
    y_upper = y_data + uncertaintiesy  # Upper bound for y at time i (for each wavelength)
    y_lower = y_data - uncertaintiesy  # Lower bound for y at time i (for each wavelength)
    
    # Polyfit for the central values (no uncertainty applied)
    m, c = np.polyfit(x, y_data, 1)
    
    # Polyfit for the upper bound data
    m_upper_value, c_upper_value = np.polyfit(x_upper, y_upper, 1)
    
    # Polyfit for the lower bound data
    m_lower_value, c_lower_value = np.polyfit(x_lower, y_lower, 1)

    # Store the values in the lists
    m_values.append(-m)
    c_values.append(c)
    m_upper.append(m_upper_value)
    m_lower.append(m_lower_value)
    c_upper.append(c_upper_value)
    c_lower.append(c_lower_value)
    
    # Calculate turbidity and its uncertainty (exp(c) and exp(c ± uncertainty))
    turbidity = np.exp(c)
    turbidity_values.append(turbidity)
    
    turbidity_upper_value = np.exp(c_upper_value)  # Upper turbidity bound (exp(c_upper))
    turbidity_lower_value = np.exp(c_lower_value)  # Lower turbidity bound (exp(c_lower))
    
    turbidity_upper.append(turbidity_upper_value)
    turbidity_lower.append(turbidity_lower_value)

# Plotting the results

# Create subplots: one row, two columns
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot Angström exponents (m values) with uncertainty bounds
ax[0].plot(time_hoursg, m_values, label='Angström Exponent', color="red", linewidth=2)
ax[0].fill_between(time_hoursg, np.array(m_values) + np.array(m_upper), np.array(m_values) - np.array(m_lower), color='red', alpha=0.3)
ax[0].set_title(f'Angström Exponent vs Time', fontsize=14)
ax[0].set_xlabel('Time (hours)', fontsize=12)
ax[0].set_ylabel('Angström Exponent', fontsize=12)
ax[0].grid(True)

# Plot Turbidity (exp(c) values) with uncertainty bounds
ax[1].plot(time_hoursg, turbidity_values, label='Turbidity', color="blue", linewidth=2)
ax[1].fill_between(time_hoursg, np.array(turbidity_upper) , np.array(turbidity_lower), color='blue', alpha=0.3)
ax[1].set_title(f'Turbidity vs Time', fontsize=14)
ax[1].set_xlabel('Time (hours)', fontsize=12)
ax[1].set_ylabel('Turbidity', fontsize=12)
ax[1].grid(True)

# Adjust layout to make sure labels and titles are clear
plt.tight_layout()

# Show the plot
plt.show()
#%%
import numpy as np
import matplotlib.pyplot as plt
import math

# Given constants (assuming aerosol optical depths and uncertainties are defined elsewhere)
x = np.array([math.log(670 * 1e-3), math.log(550.06 * 1e-3), math.log(437.71 * 1e-3)])

# Assuming 'aerosol_optical_depth_450', 'aerosol_optical_depth_550', 'aerosol_optical_depth_700'
# are 1D arrays that contain time-varying data for the 450, 550, and 700 nm wavelengths.
y = np.array([np.log(aerosol_optical_depth_450), 
              np.log(aerosol_optical_depth_550), 
              np.log(aerosol_optical_depth_700)]).T  # Transpose to match time vs wavelength

# Assume uncertainties are defined elsewhere (e.g., sigma_tau_A_450, etc.)
uncertaintiesx = np.array([math.log(20 * 1e-3), math.log(4.83 * 1e-3), math.log(19.57 * 1e-3)])
uncertaintiesy = np.array([np.log(sigma_tau_A_450), np.log(sigma_tau_A_550), np.log(sigma_tau_A_700)])

# Randomly select a time index 'i'
i = np.random.randint(0, len(y))  # Randomly select a time index
y_data = y[i]  # The y-values at this time index

# Calculate upper and lower bounds for y using uncertainties
y_upper = y_data + uncertaintiesy  # Upper bound for y
y_lower = y_data - uncertaintiesy  # Lower bound for y

# Perform the polyfit for the original y-data
m, c = np.polyfit(x, y_data, 1)

# Perform the polyfit for the upper and lower bound y-data
m_upper, c_upper = np.polyfit(x, y_upper, 1)
m_lower, c_lower = np.polyfit(x, y_lower, 1)

# Plot the data with error bars
plt.errorbar(x, y_data, yerr=np.sqrt(uncertaintiesy**2), xerr=np.sqrt(uncertaintiesx**2), fmt='o', label="Data", color="blue", capsize=5)

# Plot the original fit
plt.plot(x, m * x + c, label=f"Original Fit: y = {m:.4f}x + {c:.4f}", color="red")

# Plot the upper and lower bound fits
plt.plot(x, m_upper * x + c_upper, label=f"Upper Bound Fit: y = {m_upper:.4f}x + {c_upper:.4f}", color="green", linestyle="--")
plt.plot(x, m_lower * x + c_lower, label=f"Lower Bound Fit: y = {m_lower:.4f}x + {c_lower:.4f}", color="purple", linestyle="--")

# Add labels and title
plt.xlabel("ln(Wavelength) (ln λ)")
plt.ylabel("ln(Optical Depth) (ln τ)")
plt.title(f"Fit with Uncertainties for Time Index {i}")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Print the fit parameters
print(f"Fit Parameters for Time Index {i}:")
print(f"Slope (m): {m:.4f}, Intercept (c): {c:.4f}")
print(f"Upper Bound Fit: Slope (m): {m_upper:.4f}, Intercept (c): {c_upper:.4f}")
print(f"Lower Bound Fit: Slope (m): {m_lower:.4f}, Intercept (c): {c_lower:.4f}")



#%%
x = np.array([math.log(670*1e-3), math.log(550.06*1e-3), math.log(437.71*1e-3)])
log_y_red = np.log(aerosol_optical_depth_450)  # Taking log for each element in the red optical depth array
log_y_green = np.log(aerosol_optical_depth_550)  # Taking log for each element in the green optical depth array
log_y_blue = np.log(aerosol_optical_depth_700)  # Taking log for each element in the blue optical depth array
y = np.array([log_y_red,log_y_green,log_y_blue]).T

uncertaintiesx =  np.array([math.log(20*1e-3), math.log(4.83*1e-3), math.log(19.57*1e-3)])
u_log_y_red = np.log(sigma_tau_A_450)  # Taking log for each element in the red optical depth array
u_log_y_green = np.log(sigma_tau_A_550)  # Taking log for each element in the green optical depth array
u_log_y_blue = np.log(sigma_tau_A_700)  # Taking log for each element in the blue optical depth array
uncertaintiesy =  np.array([u_log_y_red,u_log_y_green,u_log_y_blue]).T

angstroms=[]
turbidities=[]
for i in y:
    m, c = np.polyfit(x,i,1)
    angstroms.append(-m)
    turbidities.append(np.exp(c))

# Create subplots: one row, two columns
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot Angström exponents on the first subplot
ax[0].plot(time_hoursg, angstroms, color="red", linewidth=2)
ax[0].set_title(f'Angström Exponent vs Time (mean = {np.mean(angstroms):.2f})', fontsize=14)
ax[0].set_xlabel('Time (hours)', fontsize=12)
ax[0].set_ylabel('Angström Exponent', fontsize=12)
ax[0].grid(True)

# Plot Turbidity on the second subplot
ax[1].plot(time_hoursg, turbidities, color="blue", linewidth=2)
ax[1].set_title(f'Turbidity vs Time (mean = {np.mean(turbidities):.2f})', fontsize=14)
ax[1].set_xlabel('Time (hours)', fontsize=12)
ax[1].set_ylabel('Turbidity', fontsize=12)
ax[1].grid(True)

# Adjust layout to make sure labels and titles are clear
plt.tight_layout()

# Show the plot
plt.show()

#%%
import random

# Randomly select an index for the data
choice = random.randint(0, len(y) - 1)  # Ensure the index is within bounds of y

# Fit a line to the chosen data with covariance matrix
(m, c), cov = np.polyfit(x, y[choice], 1, cov=True)

# Extract standard errors (uncertainties) from covariance matrix
m_uncertainty = np.sqrt(cov[0, 0])  # Standard deviation of slope
c_uncertainty = np.sqrt(cov[1, 1])  # Standard deviation of intercept

# Plot the data and the linear fit
plt.scatter(x, y[choice], label="Data", color="blue")
plt.plot(x, m * x + c, label=f"Fit: y = {m:.4f}x + {c:.4f}", color="red")

# Display the Ångström coefficient and turbidity with uncertainties
plt.text(min(x), max(y[choice]) - 0.1, f"Ångström Coeff: {-m:.4f} ± {m_uncertainty:.4f}", fontsize=10, color="black")
plt.text(min(x), max(y[choice]) - 0.2, f"Turbidity: {np.exp(c):.4f} ± {np.exp(c) * c_uncertainty:.4f}", fontsize=10, color="black")

# Labeling and title
plt.xlabel("ln(Wavelength) (ln λ)")
plt.ylabel("ln(Optical Depth) (ln τ)")
plt.title(f"Ångström Law Fit Time: {choice / 3600 + 13.8333:.4f} hours")
plt.legend()
plt.grid(True)

# Show the plot
plt.show()

# Print uncertainties for reference
print(f"Ångström Coefficient: {-m:.4f} ± {m_uncertainty:.4f}")
print(f"Turbidity: {np.exp(c):.4f} ± {np.exp(c) * c_uncertainty:.4f}")

#%%

import numpy as np
import matplotlib.pyplot as plt
import math
from scipy import stats

# Given constants (assuming aerosol optical depths and uncertainties are defined elsewhere)
x = np.array([math.log(670 * 1e-3), math.log(550.06 * 1e-3), math.log(437.71 * 1e-3)])

# Assuming 'aerosol_optical_depth_450', 'aerosol_optical_depth_550', 'aerosol_optical_depth_700'
# are 1D arrays that contain time-varying data for the 450, 550, and 700 nm wavelengths.
y = np.array([np.log(aerosol_optical_depth_450), 
              np.log(aerosol_optical_depth_550), 
              np.log(aerosol_optical_depth_700)]).T  # Transpose to match time vs wavelength

# Assume uncertainties are defined elsewhere (e.g., sigma_tau_A_450, etc.)
uncertaintiesx = np.array([math.log(2), math.log(0.483), math.log(1.957)])
uncertaintiesy = np.array([np.log(sigma_tau_A_450), np.log(sigma_tau_A_550), np.log(sigma_tau_A_700)])

# Randomly select a time index 'i'
i = np.random.randint(0, len(y))  # Randomly select a time index
y_data = y[i]  # The y-values at this time index

# Calculate upper and lower bounds for y using uncertainties
y_upper = y_data + uncertaintiesy  # Upper bound for y
y_lower = y_data - uncertaintiesy  # Lower bound for y

# Compute the weights for the weighted least squares fit
weights_x = 1 / uncertaintiesx  # Inverse of uncertainties in x
weights_y = 1 / uncertaintiesy  # Inverse of uncertainties in y

# We need to propagate uncertainties in both x and y.
# The total weight for each data point is a combination of x and y uncertainties.
weights = 1 / (weights_x**2 + weights_y**2)  # Combined weight for each point

# Perform the weighted least squares regression
# Weights are applied to the y-values
A = np.vstack([x, np.ones(len(x))]).T  # Design matrix for linear model y = mx + c
w = np.diag(weights)  # Diagonal matrix of weights

# Compute the weighted fit parameters (m and c)
fit_params = np.linalg.inv(A.T @ w @ A) @ A.T @ w @ y_data
m = fit_params[0]
c = fit_params[1]

# Perform the weighted least squares for upper and lower bound y-data
fit_params_upper = np.linalg.inv(A.T @ w @ A) @ A.T @ w @ y_upper
fit_params_lower = np.linalg.inv(A.T @ w @ A) @ A.T @ w @ y_lower

# Extract slope (m) and intercept (c) for the upper and lower bounds
m_upper, c_upper = fit_params_upper[0], fit_params_upper[1]
m_lower, c_lower = fit_params_lower[0], fit_params_lower[1]

# Plot the data with error bars
plt.errorbar(x, y_data, yerr=np.sqrt(uncertaintiesy**2), xerr=np.sqrt(uncertaintiesx**2), fmt='o', label="Data", color="blue", capsize=5)

# Plot the original fit
plt.plot(x, m * x + c, label=f"Original Fit: y = {m:.4f}x + {c:.4f}", color="red")

# Plot the upper and lower bound fits
plt.plot(x, m_upper * x + c_upper, label=f"Upper Bound Fit: y = {m_upper:.4f}x + {c_upper:.4f}", color="green", linestyle="--")
plt.plot(x, m_lower * x + c_lower, label=f"Lower Bound Fit: y = {m_lower:.4f}x + {c_lower:.4f}", color="purple", linestyle="--")

# Add labels and title
plt.xlabel("ln(Wavelength) (ln λ)")
plt.ylabel("ln(Optical Depth) (ln τ)")
plt.title(f"Fit with Uncertainties for Time Index {i}")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Print the fit parameters
print(f"Fit Parameters for Time Index {i}:")
print(f"Slope (m): {m:.4f}, Intercept (c): {c:.4f}")
print(f"Upper Bound Fit: Slope (m): {m_upper:.4f}, Intercept (c): {c_upper:.4f}")
print(f"Lower Bound Fit: Slope (m): {m_lower:.4f}, Intercept (c): {c_lower:.4f}")


#%%


#import math UNSCALED
#Average Optical Depth for Green Filter: 2.6944
#Average Optical Depth for Red Filter: 3.0514
#Average Optical Depth for Blue Filter: 2.1456

#x = np.array([-14.21598812, -14.41323847, -14.64170925])
#y = np.array([1.115600502, 0.9911755451, 0.763412335])

#SCALED
#Average Optical Depth for Green Filter: -0.0877
#Average Optical Depth for Red Filter: 3.0514
#Average Optical Depth for Blue Filter: 0.2729

#x = np.array([-14.21598812, -14.64170925])
#y = np.array([1.115600502, -1.2986])

#plot ln(optical depth) = ln(turbidity) - angstrom(ln(wavelength))
#red": 670, "green": 550.06, "blue": 437.71}
#red = ln(670), green = ln()

#x = ln(lambda)
#y = ln(optical depth)
