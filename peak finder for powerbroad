# -*- coding: utf-8 -*-
"""
Created on Mon Apr  7 13:44:20 2025

@author: henry
"""

import os
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter  # Changed from ScientificNotation
from scipy.signal import savgol_filter
from scipy.optimize import curve_fit
from scipy.signal import find_peaks

# Constants for easy adjustment
DEFAULT_XLIM = (0.068, 0.082)  # X-axis limits
MOVING_AVG_WINDOW_SIZE = 1500  # Window size for moving average
SAVGOL_WINDOW_SIZE = 1500      # Window size for Savitzky-Golay filter
SAVGOL_POLY_ORDER = 3          # Polynomial order for Savitzky-Golay filter
BACKGROUND_SCALE = 0.65        # Scaling factor for background subtraction

# Gaussian model definitions
def two_gaussians(x, A1, mu1, sigma1, A2, mu2, sigma2):
    """
    Model function for two Gaussian peaks.
    
    Parameters:
    -----------
    x : array-like
        X-axis values
    A1, A2 : float
        Amplitudes of the two Gaussians
    mu1, mu2 : float
        Center positions of the two Gaussians
    sigma1, sigma2 : float
        Standard deviations of the two Gaussians
        
    Returns:
    --------
    array-like
        Combined Gaussian function values
    """
    return A1 * np.exp(-(x - mu1)**2 / (2 * sigma1**2)) + A2 * np.exp(-(x - mu2)**2 / (2 * sigma2**2))

def load_data_files(folder_path, extension=".CSV"):
    """
    Load all CSV files in a folder into a dictionary of DataFrames.
    
    Parameters:
    -----------
    folder_path : str
        Path to the folder containing data files
    extension : str, optional
        File extension to look for (default: ".CSV")
        
    Returns:
    --------
    dict
        Dictionary with power values as keys and DataFrames as values
    """
    # Dictionary to store DataFrames
    df_dict = {}
    
    try:
        # Get list of all matching files in the folder
        data_files = [f for f in os.listdir(folder_path) if f.endswith(extension)]
        
        if not data_files:
            print(f"No {extension} files found in {folder_path}")
            return df_dict
        
        # Process each file
        for file in data_files:
            file_path = os.path.join(folder_path, file)
            
            try:
                # Try different delimiters if the default fails
                try:
                    df = pd.read_csv(file_path)
                except:
                    # Try with different delimiters
                    for delimiter in [',', '\t', ';', ' ']:
                        try:
                            df = pd.read_csv(file_path, delimiter=delimiter)
                            if len(df.columns) >= 2:  # Ensure at least two columns
                                break
                        except:
                            continue
                
                # Extract the power value from filename
                power_match = re.match(r"([\d\.]+)uW", file)
                if power_match:
                    power_key = power_match.group(1)
                    
                    # Rename columns for clarity if they don't have proper names
                    if df.columns[0] == '0' or df.columns[0].isdigit():
                        df.columns = ['Wavelength', 'Intensity']
                    
                    df_dict[power_key] = df
                    print(f"Loaded: {file} (Power: {power_key}uW)")
            
            except Exception as e:
                print(f"Error loading {file}: {str(e)}")
                
        return df_dict
    
    except Exception as e:
        print(f"Error accessing folder {folder_path}: {str(e)}")
        return df_dict

def process_all_datasets(folder_path, output_folder=None):
    """
    Process all datasets in a folder.
    
    Parameters:
    -----------
    folder_path : str
        Path to the folder containing data files
    output_folder : str, optional
        Path to save output files (default: None, creates subfolder in folder_path)
    """
    # Load all data files
    print(f"Loading data from {folder_path}...")
    df_dict = load_data_files(folder_path)
    
    return df_dict

def main():
    """Main function to run the spectroscopy data analysis."""
    # Set folder path
    folder_path = input("Enter the folder path containing CSV files: ")
    
    if not os.path.exists(folder_path):
        print(f"Error: Folder {folder_path} does not exist.")
        return
    
    # Process all datasets
    df_dict = process_all_datasets(folder_path)
    
    print("\nData processing complete!")
    return df_dict
    

if __name__ == "__main__":
    # Set random seed for reproducibility
    np.random.seed(42)
    
    # Call the main function
    df_dict = main()
    

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter
from scipy import sparse
from scipy.sparse.linalg import spsolve

def baseline_als(y, lam=1e7, p=0.000005, niter=10):
    """
    Asymmetric Least Squares Baseline estimation
    
    Input:
        y: signal
        lam: smoothness parameter (Î»), higher means smoother
        p: asymmetry parameter (p), smaller means more asymmetric
        niter: number of iterations
    
    Output:
        baseline: estimated baseline
    """
    L = len(y)
    D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))
    D = lam * D.dot(D.transpose())
    
    w = np.ones(L)
    W = sparse.spdiags(w, 0, L, L)
    
    for i in range(niter):
        W.setdiag(w)
        Z = W + D
        z = spsolve(Z, w*y)
        w = p * (y > z) + (1-p) * (y < z)
    
    return z

def filter_data_range(df, x_min=0.068, x_max=0.082):
    """Filter dataframe to keep only data within the specified x range"""
    filtered_df = df[(df.iloc[:, 0] >= x_min) & (df.iloc[:, 0] <= x_max)].copy()
    return filtered_df

def extract_fine_peaks_hybrid(df, lam=1e7, p=0.000005, smooth_window=1501, x_min=0.068, x_max=0.082):
    """Hybrid approach combining smoothing and ALS baseline correction"""
    # Filter data to specified range
    df = filter_data_range(df, x_min, x_max)
    
    x_data = df.iloc[:, 0].values
    y_data = df.iloc[:, 1].values
    
    # Adjust window size to be odd and less than data length
    if len(y_data) < smooth_window:
        smooth_window = min(len(y_data) // 2 * 2 - 1, 51)  # Ensure odd number and reasonable size
    
    # Step 1: Apply Savitzky-Golay smoothing to reduce noise
    y_smooth = savgol_filter(y_data, window_length=smooth_window, polyorder=3)
    
    # Step 2: Apply ALS baseline correction to the smoothed data
    baseline = baseline_als(y_smooth, lam=lam, p=p)
    
    # Step 3: Subtract baseline from original (unsmoothed) data
    fine_peaks = y_data - baseline
    
    # Plot results
    fig, axes = plt.subplots(3, 1, figsize=(10, 12))
    
    # Top plot: Original data with smoothed version
    axes[0].plot(x_data, y_data, label="Original Data", color='black', alpha=0.7)
    axes[0].plot(x_data, y_smooth, label="Smoothed Data", color='blue')
    axes[0].set_title(f"Original and Smoothed Data (X range: {x_min}-{x_max})")
    axes[0].legend()
    
    # Middle plot: Smoothed data with baseline
    axes[1].plot(x_data, y_smooth, label="Smoothed Data", color='blue')
    axes[1].plot(x_data, baseline, label="ALS Baseline", color='red')
    axes[1].set_title("Smoothed Data with ALS Baseline")
    axes[1].legend()
    
    # Bottom plot: Background-subtracted data
    axes[2].plot(x_data, fine_peaks, label="Fine Peaks", color='green')
    axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[2].set_title("Background-Subtracted Fine Peaks")
    
    plt.tight_layout()
    plt.show()
    
    return pd.DataFrame({'X': x_data, 'Fine_Peaks': fine_peaks})

def process_all_datasets_hybrid(df_dict, lam=1e7, p=0.000005, smooth_window=1501, x_min=0.068, x_max=0.082, plot_each=False):
    """
    Process all datasets with the hybrid method using the specified parameters
    
    Parameters:
        df_dict: Dictionary of dataframes containing the datasets
        lam: Smoothness parameter for ALS baseline
        p: Asymmetry parameter for ALS baseline
        smooth_window: Window size for Savitzky-Golay filter
        x_min, x_max: X-range to analyze
        plot_each: Whether to plot each individual dataset processing
    
    Returns:
        DataFrame with X values and Fine_Peaks for all datasets
    """
    all_fine_peaks = {}
    common_x = None
    
    for key in df_dict.keys():
        print(f"Processing dataset {key}...")
        
        # Filter data to specified range
        filtered_df = filter_data_range(df_dict[key], x_min, x_max)
        
        x_data = filtered_df.iloc[:, 0].values
        y_data = filtered_df.iloc[:, 1].values
        
        # Store x values for the first dataset
        if common_x is None:
            common_x = x_data
        
        # Adjust window size if needed
        actual_smooth_window = smooth_window
        if len(y_data) < smooth_window:
            actual_smooth_window = min(len(y_data) // 2 * 2 - 1, 51)
            print(f"  Adjusted smooth_window to {actual_smooth_window} for dataset {key}")
        
        # Apply Savitzky-Golay smoothing
        y_smooth = savgol_filter(y_data, window_length=actual_smooth_window, polyorder=2)
        
        # Apply ALS baseline correction
        baseline = baseline_als(y_smooth, lam=lam, p=p)
        
        # Subtract baseline from original data
        fine_peaks = y_data - baseline
        
        # Store results
        all_fine_peaks[key] = fine_peaks
        
        # Plot if requested
        if plot_each:
            fig, axes = plt.subplots(3, 1, figsize=(10, 12))
            
            axes[0].plot(x_data, y_data, label="Original Data", color='black', alpha=0.7)
            axes[0].plot(x_data, y_smooth, label="Smoothed Data", color='blue')
            axes[0].set_title(f"Dataset {key}: Original and Smoothed Data")
            axes[0].legend()
            
            axes[1].plot(x_data, y_smooth, label="Smoothed Data", color='blue')
            axes[1].plot(x_data, baseline, label="ALS Baseline", color='red')
            axes[1].set_title(f"Dataset {key}: Smoothed Data with ALS Baseline")
            axes[1].legend()
            
            axes[2].plot(x_data, fine_peaks, label="Fine Peaks", color='green')
            axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
            axes[2].set_title(f"Dataset {key}: Background-Subtracted Fine Peaks")
            
            plt.tight_layout()
            plt.show()
    
    # Create DataFrame with x-values and all fine peaks
    results_df = pd.DataFrame({'X': common_x})
    
    # Add column for each dataset
    for key, peaks in all_fine_peaks.items():
        results_df[f"{key}"] = peaks
    
    # Save results to CSV
    filename = f"fine_peaks_hybrid_all_datasets_x_{x_min}_{x_max}.csv"
    results_df.to_csv(filename, index=False)
    print(f"Results saved to {filename}")
    
    # Plot all fine peaks together
    plt.figure(figsize=(12, 8))
    for key, peaks in all_fine_peaks.items():
        plt.plot(common_x, peaks, label=f"Dataset {key}")
    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    plt.title("Fine Peaks for All Datasets")
    plt.xlabel("X")
    plt.ylabel("Fine Peaks")
    plt.legend()
    plt.tight_layout()
    plt.show()
    
    return results_df

process_all_datasets_hybrid(df_dict, lam=1e7, p=0.000005, smooth_window=1501, x_min=0.068, x_max=0.082, plot_each=True)

#%%
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.signal import find_peaks, peak_widths
import os
import glob

def import_fine_peaks_data(filepath=None):
    """
    Import fine peaks data from a CSV file.
    If filepath is None, look for the most recent fine_peaks*.csv file in the current directory.
    
    Returns:
        DataFrame containing the fine peaks data
    """
    if filepath is None:
        # Find the most recent file matching the pattern
        files = glob.glob('fine_peaks*.csv')
        if not files:
            raise FileNotFoundError("No fine_peaks*.csv files found in the current directory")
        
        # Sort by modification time (newest first)
        files.sort(key=os.path.getmtime, reverse=True)
        filepath = files[0]
        print(f"Using most recent file: {filepath}")
    
    # Import the data
    try:
        data = pd.read_csv(filepath)
        print(f"Successfully imported data with columns: {data.columns.tolist()}")
        return data
    except Exception as e:
        print(f"Error importing data: {str(e)}")
        raise

def find_and_analyze_peaks(data, height_threshold=0.5, prominence=0.5, width_threshold=None, 
                          show_all_peaks=True, expected_peaks=None):
    """
    Find and analyze peaks in fine peaks data.
    
    Parameters:
        data: DataFrame with 'X' column and multiple data columns
        height_threshold: Minimum peak height
        prominence: Minimum peak prominence
        width_threshold: Maximum peak width (None for no limit)
        show_all_peaks: Whether to plot all peaks or only those meeting criteria
        expected_peaks: List of expected peak positions for reference lines
        
    Returns:
        Dictionary containing peak information for each dataset
    """
    # Process each dataset column (excluding the X column)
    peak_results = {}
    x_values = data['X'].values
    
    # Create a figure for the overall comparison
    plt.figure(figsize=(15, 10))
    
    # Set up color cycle
    colors = plt.cm.tab10(np.linspace(0, 1, len(data.columns) - 1))
    color_idx = 0
    
    # Process each dataset
    for column in data.columns:
        if column == 'X':
            continue
            
        # Get y values for this dataset
        y_values = data[column].values
        
        # Find peaks
        peaks, properties = find_peaks(y_values, height=height_threshold, prominence=prominence)
        
        # Calculate peak widths (returns tuple of width, width_heights, left_ips, right_ips)
        widths_result = peak_widths(y_values, peaks, rel_height=0.5)
        widths = widths_result[0]
        
        # Filter peaks by width if threshold is specified
        if width_threshold is not None:
            valid_peaks_mask = widths <= width_threshold
            peaks = peaks[valid_peaks_mask]
            widths = widths[valid_peaks_mask]
            heights = properties['peak_heights'][valid_peaks_mask]
            prominences = properties['prominences'][valid_peaks_mask]
        else:
            heights = properties['peak_heights']
            prominences = properties['prominences']
        
        # Convert peak indices to x values
        peak_positions = x_values[peaks]
        
        # Store results
        peak_results[column] = {
            'positions': peak_positions,
            'heights': heights,
            'widths': widths,
            'prominences': prominences
        }
        
        # Plot data and peaks
        plt.plot(x_values, y_values, color=colors[color_idx], alpha=0.6, label=f"{column} data")
        
        # Plot detected peaks
        if show_all_peaks:
            plt.plot(peak_positions, heights, 'x', color=colors[color_idx], 
                    markersize=10, label=f"{column} peaks")
        
        # Add peak position annotations
        for i, pos in enumerate(peak_positions):
            plt.annotate(f"{pos:.5f}", (pos, heights[i]), 
                        xytext=(0, 10), textcoords='offset points',
                        ha='center', fontsize=8, color=colors[color_idx])
        
        color_idx += 1
    
    # Plot expected peak positions if provided
    if expected_peaks is not None:
        for pos in expected_peaks:
            plt.axvline(x=pos, color='k', linestyle='--', alpha=0.3)
            plt.annotate(f"Expected: {pos:.5f}", (pos, 0), 
                        xytext=(0, -20), textcoords='offset points',
                        ha='center', fontsize=8, rotation=90)
    
    plt.title("Fine Peaks Analysis")
    plt.xlabel("X")
    plt.ylabel("Intensity")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    # Detailed analysis report for each dataset
    for column, results in peak_results.items():
        print(f"\nPeak Analysis for {column}:")
        print(f"{'Position':<10} {'Height':<10} {'Width':<10} {'Prominence':<10}")
        print("-" * 40)
        for i in range(len(results['positions'])):
            print(f"{results['positions'][i]:.5f}  {results['heights'][i]:.5f}  "
                  f"{results['widths'][i]:.5f}  {results['prominences'][i]:.5f}")
    
    return peak_results

def analyze_peak_statistics(peak_results):
    """
    Analyze statistics of peak positions across datasets.
    
    Parameters:
        peak_results: Dictionary from find_and_analyze_peaks
        
    Returns:
        DataFrame with peak statistics
    """
    # Collect all unique peak positions
    all_positions = []
    for dataset, results in peak_results.items():
        all_positions.extend(results['positions'])
    
    # Cluster similar peak positions (within 0.0005 of each other)
    tolerance = 0.0005
    clustered_peaks = []
    
    for pos in sorted(all_positions):
        # Check if this position is close to any existing cluster
        found_cluster = False
        for cluster in clustered_peaks:
            if abs(pos - np.mean(cluster)) < tolerance:
                cluster.append(pos)
                found_cluster = True
                break
        
        # If not close to existing cluster, create new one
        if not found_cluster:
            clustered_peaks.append([pos])
    
    # Calculate statistics for each cluster
    peak_stats = []
    for i, cluster in enumerate(clustered_peaks):
        mean_pos = np.mean(cluster)
        std_pos = np.std(cluster)
        count = len(cluster)
        datasets = []
        
        # Find which datasets have this peak
        for dataset, results in peak_results.items():
            for pos in results['positions']:
                if abs(pos - mean_pos) < tolerance:
                    datasets.append(dataset)
                    break
        
        peak_stats.append({
            'Peak_Number': i+1,
            'Mean_Position': mean_pos,
            'Std_Dev': std_pos,
            'Count': count,
            'Present_In': ', '.join(datasets)
        })
    
    # Create DataFrame
    stats_df = pd.DataFrame(peak_stats)
    
    # Display summary
    print("\nPeak Position Statistics:")
    print(stats_df.to_string(index=False))
    
    # Create a visualization of peak presence across datasets
    plt.figure(figsize=(12, 6))
    
    datasets = sorted(peak_results.keys())
    peak_means = [stat['Mean_Position'] for stat in peak_stats]
    
    # Create a matrix of peak presence (1 if present, 0 if not)
    presence_matrix = np.zeros((len(datasets), len(peak_means)))
    
    for i, dataset in enumerate(datasets):
        for j, mean_pos in enumerate(peak_means):
            # Check if dataset has a peak near this position
            has_peak = False
            for pos in peak_results[dataset]['positions']:
                if abs(pos - mean_pos) < tolerance:
                    has_peak = True
                    break
            presence_matrix[i, j] = 1 if has_peak else 0
    
    # Plot presence matrix as a heatmap
    plt.imshow(presence_matrix, aspect='auto', cmap='viridis')
    plt.colorbar(label='Peak Present')
    plt.xlabel('Peak Number')
    plt.ylabel('Dataset')
    plt.title('Peak Presence Across Datasets')
    plt.xticks(range(len(peak_means)), [f"{i+1}\n({p:.5f})" for i, p in enumerate(peak_means)])
    plt.yticks(range(len(datasets)), datasets)
    plt.grid(False)
    plt.tight_layout()
    plt.show()
    
    return stats_df

def main():
    """Main function to run the analysis"""
    # Ask user for file path or use the most recent
    use_most_recent = input("Use most recent fine_peaks CSV file? (y/n): ").lower().startswith('y')
    
    if use_most_recent:
        filepath = None
    else:
        filepath = input("Enter the path to the fine_peaks CSV file: ")
        if not filepath:
            print("No filepath provided, using most recent file.")
            filepath = None
    
    # Import data
    data = import_fine_peaks_data(filepath)
    
    # Get user input for peak finding parameters
    height_threshold = float(input("Enter minimum peak height (default 0.5): ") or 0.5)
    prominence = float(input("Enter minimum peak prominence (default 0.5): ") or 0.5)
    
    use_width = input("Apply width filter? (y/n): ").lower().startswith('y')
    width_threshold = None
    if use_width:
        width_threshold = float(input("Enter maximum peak width: "))
    
    # Ask about expected peaks
    have_expected = input("Do you have expected peak positions? (y/n): ").lower().startswith('y')
    expected_peaks = None
    if have_expected:
        expected_input = input("Enter expected peak positions separated by commas: ")
        if expected_input:
            expected_peaks = [float(x.strip()) for x in expected_input.split(',')]
    
    # Find and analyze peaks
    peak_results = find_and_analyze_peaks(data, height_threshold, prominence, 
                                         width_threshold, True, expected_peaks)
    
    # Analyze peak statistics
    stats_df = analyze_peak_statistics(peak_results)
    
    # Save results
    save_results = input("Save peak statistics to CSV? (y/n): ").lower().startswith('y')
    if save_results:
        output_file = input("Enter output filename (default: peak_statistics.csv): ") or "peak_statistics.csv"
        stats_df.to_csv(output_file, index=False)
        print(f"Statistics saved to {output_file}")

if __name__ == "__main__":
    main()
